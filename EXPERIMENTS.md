# Experiments Report (HRDE_RAG.ipynb)

This document describes the experiments executed in `HRDE_RAG.ipynb`, including:
- What settings were used
- How many runs were executed
- What metrics were computed
- The **evaluation results** (pulled from `eval/summary.json`)

> Important: The notebook computes metrics and writes them to `eval/summary.json`.  
> Copy the exact content of that file into the “Results” section below after running the evaluation cell.

---

## 1) Goal

Evaluate an HRDE-style hybrid RAG pipeline for Chinese health rumor verification using:
- Hybrid retrieval (FAISS + BM25)
- Optional HyDE reranking
- A local Hugging Face LLM for final verdict generation

---

## 2) Data

### Reference corpus
- Source: Hugging Face dataset `Hush-cd/HealthRCN` (streamed via datasets-server)
- The notebook writes a normalized JSONL file to:
  - `data/reference_docs.jsonl`
- Default build size in notebook:
  - **1,000** reference items

### Evaluation set
- The notebook samples from the dataset and writes:
  - `data/eval_samples.jsonl`
- The evaluation set is **balanced** (equal rumor / non-rumor), using a fixed seed if present in the notebook.

---

## 3) Retrieval + indexing configuration

### Chunking
- `max_chars = 800`
- `overlap = 120`

### Embeddings
- Sentence embedding model (default in notebook): `moka-ai/m3e-base`

### Retrieval
- FAISS top-n: `TOP_N_FAISS` (default: 12)
- BM25 top-n: `TOP_N_BM25` (default: 12)
- Final evidence size: `FINAL_K` (default: 6)
- Similarity threshold: `sim_threshold` (default: 0.3)

### Optional HyDE
- `USE_HYDE` (default: True in evaluation section)

---

## 4) Generation configuration

### Local LLM
- Hugging Face model folder: `MODEL_DIR`
- Default model ID in notebook:
  - `Qwen/Qwen3-4B-Instruct-2507`

### Evidence length control
- Evidence snippets are truncated to reduce context length:
  - `MAX_EVID_CHARS` (default in notebook: 260)

---

## 5) Experiment protocol

### Number of runs
- Evaluation is executed **once** over the sampled evaluation set.

If you want multiple runs:
- Re-run evaluation with different random seeds and compare `eval/summary.json` across runs.

### Artifacts saved
- Per-sample outputs:
  - `eval/per_sample.jsonl`
- Final metrics:
  - `eval/summary.json`
- Any plots generated by the notebook:
  - `eval/*.png` (or similar)

---

## 6) Metrics

The notebook computes:

- `n_total`: total evaluation samples
- `n_valid`: number of samples where label parsing succeeded
- `valid_rate = n_valid / n_total`
- `accuracy` and `f1` computed on valid samples only

Optional RRR judging:
- `relevance_avg` (0–1 scale after normalization)
- `reliability_avg` (0–1)
- `richness_avg` (0–1)
- `rrr_n_scored`

---

## 7) Evaluation results (from `eval/summary.json`)

Paste the content of `eval/summary.json` produced by your run here:

```json
{
  "n_total": 200,
  "n_valid": 166,
  "valid_rate": 0.83,
  "accuracy": 0.7228915662650602,
  "f1": 0.7946428571428571,
  "relevance_avg": 0.969,
  "reliability_avg": 0.908,
  "richness_avg": 0.9005000000000001,
  "rrr_n_scored": 200
}
```

### Short interpretation template
- **Valid rate:** (higher is better) indicates parsing stability of the verdict format.
- **Accuracy / F1:** main classification performance on parseable outputs.
- **RRR averages:** quality of explanation/evidence grounding (only if judge parsing succeeds).

---

## 8) Reproducibility notes

To reproduce results:
1. Run the notebook **from a clean runtime**.
2. Keep the same:
   - reference docs build size
   - chunking parameters
   - embedding model
   - retrieval top-n and `FINAL_K`
   - `USE_HYDE` setting
3. Save and attach:
   - `eval/summary.json`
   - `eval/per_sample.jsonl`
